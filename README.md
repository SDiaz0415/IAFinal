# :car: Chatbot RAG para Fichas TÃ©cnicas de AutomÃ³viles
Este proyecto implementa un chatbot especializado en fichas tÃ©cnicas de automÃ³viles, utilizando la arquitectura RAG (Retrieval Augmented Generation). Combina la potencia de un modelo de lenguaje grande con la capacidad de recuperar informaciÃ³n tÃ©cnica precisa de documentos PDF cargados por el usuario, generando respuestas contextualizadas. Desarrollado con Streamlit, LangChain y Ollama.

---

### :file_folder: Estructura del Proyecto
```
ğŸ“ backend
|â”€â”€ ğŸ“ app
â”‚   â”œâ”€â”€ ğŸ“ api
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â””â”€â”€ schemas.py
â”‚   â”œâ”€â”€ ğŸ“ core
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ exceptions.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ ğŸ“ data_processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ text_extractor_txt.py
â”‚   â”œâ”€â”€ ğŸ“ embeddings
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chroma_db
â”‚   â”‚   â”œâ”€â”€ FAISS_db.py
â”‚   â”‚   â”œâ”€â”€ milvus_db.py
â”‚   â”‚   â”œâ”€â”€ Qdrant_db.py
â”‚   â”‚   â””â”€â”€ service.py
â”‚   â”œâ”€â”€ ğŸ“ llm
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ clients.py
â”‚   â”‚   â”œâ”€â”€ historical.py
â”‚   â”‚   â”œâ”€â”€ manager.py
â”‚   â”‚   â”œâ”€â”€ prompts.py
â”‚   â”‚   â””â”€â”€ rag.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ ğŸ“ data
â”‚   â”œâ”€â”€ 202411-01-sail.pdf
â”‚   â”œâ”€â”€ 202411-03-onix-turbo.pdf
â”‚   â””â”€â”€ 202411-08-tahoe.pdf
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ uv.lock
ğŸ“ frontend
|â”€â”€ ğŸ“ config
â”‚   â”œâ”€â”€ config.toml 
â”‚â”€â”€ ğŸ“ public
â”‚   â”œâ”€â”€ favicon.png
â”‚   â”œâ”€â”€ logo_dark.png
â”‚   â”œâ”€â”€ logo_light.png
â”‚   â””â”€â”€ logo.png
â”œâ”€â”€ __init__.py
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ main.py
â”œâ”€â”€ Makefile
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ uv.lock
```
---

## :dart: Prerrequisitos

Antes de iniciar el proyecto, asegÃºrate de tener instalado lo siguiente:

* **uv**: Un administrador de paquetes de Python rÃ¡pido, escrito en Rust. Lo usaremos para gestionar las dependencias del proyecto.
    * Sitio web oficial: (https://docs.astral.sh/uv/getting-started/installation/)
* **Ollama**: Una herramienta para ejecutar modelos de lenguaje grandes localmente. NecesitarÃ¡s descargar e instalar Ollama para poder usar los modelos de IA.
    * Sitio web oficial: (https://ollama.com/)
* **Chocolatey** (Solo para Windows): Un administrador de paquetes para Windows. Si usas Windows, Chocolatey facilita la instalaciÃ³n de herramientas como `make`.
    * Sitio web oficial: (https://chocolatey.org/install)
* **make**: Una utilidad que controla la generaciÃ³n de ejecutables y otros archivos a partir de los archivos fuente de un programa. Lo usaremos para simplificar el proceso de inicio del proyecto.

## InstalaciÃ³n de Prerrequisitos

Sigue las instrucciones de instalaciÃ³n de los sitios web oficiales para cada herramienta segÃºn tu sistema operativo:

* **uv**: Consulta la secciÃ³n de instalaciÃ³n en el sitio web oficial de uv.
* **Ollama**: Descarga e instala la versiÃ³n adecuada para tu sistema operativo desde el sitio web oficial de Ollama.
* **Chocolatey** (Solo para Windows): Sigue las instrucciones de instalaciÃ³n en el sitio web oficial de Chocolatey.
* **make**:
    * En sistemas basados en Debian/Ubuntu, puedes instalarlo con `sudo apt update && sudo apt install make`.
    * En macOS, generalmente viene incluido con las herramientas de desarrollo Xcode. Si no lo tienes, puedes instalarlas ejecutando `xcode-select --install`.
    * En Windows, si instalaste Chocolatey, puedes instalar make ejecutando `choco install make`.
---

## :hammer_and_wrench: ConfiguraciÃ³n

Una vez que tengas Ollama instalado, necesitarÃ¡s descargar un modelo de lenguaje grande y el modelo de embeddings `nomic-embed-text`.

1.  **Descargar un modelo de IA**: Elige un modelo de la librerÃ­a de Ollama ([https://ollama.com/library](https://ollama.com/library)) y descÃ¡rgalo usando el comando `ollama pull <nombre_del_modelo>`. Por ejemplo:
    ```bash
    ollama run llama3.2
    ```
    (Puedes reemplazar `llama3.2` con el nombre del modelo que prefieras).

2.  **Descargar el modelo de embeddings `nomic-embed-text`**: Este modelo es necesario para el componente de recuperaciÃ³n del RAG.
    ```bash
    ollama pull nomic-embed-text
    ```
    * PÃ¡gina del modelo `nomic-embed-text` en la librerÃ­a de Ollama: [https://ollama.com/library/nomic-embed-text](https://ollama.com/library/nomic-embed-text)

3. **ğŸš¨ Requisito Obligatorio**: Antes de ejecutar este proyecto, **debes crear una cuenta gratuita o de pago en [ConvertAPI](https://www.convertapi.com/)**. Esta cuenta es indispensable para generar tu **endpoint** y **secreto de autenticaciÃ³n**, los cuales se utilizarÃ¡n en las funciones de conversiÃ³n de archivos.

---

## ğŸ§   Bases de Datos Vectoriales Usadas

Para la indexaciÃ³n y bÃºsqueda semÃ¡ntica de informaciÃ³n, este proyecto es compatible con las siguientes bases de datos vectoriales:

| Base de Datos | Tipo | DescripciÃ³n |
|---------------|------|-------------|
| **Chroma**    | Local | Ideal para desarrollo local o proyectos pequeÃ±os. FÃ¡cil de configurar. |
| **FAISS**     | Local | Potente y eficiente para grandes volÃºmenes, desarrollado por Facebook. |
| **Milvus**    | Nube | Altamente escalable, recomendada para producciÃ³n. [Sitio oficial](https://milvus.io/es) |
| **Qdrant**    | Nube | Basada en Rust, ofrece alta performance. [Sitio oficial](https://qdrant.tech/) |

---

### â˜ï¸ Consideraciones para Bases Vectoriales en la Nube

Para utilizar **Milvus** o **Qdrant** en la nube:

1. **Crea una cuenta en los portales oficiales:**
   - [Milvus Cloud](https://milvus.io/es)
   - [Qdrant Cloud](https://qdrant.tech/)

2. **Crea un clÃºster** con el nombre que desees desde el panel de administraciÃ³n.

3. Una vez creado, obtendrÃ¡s un **endpoint HTTP** y un **token de API** que deberÃ¡s configurar en el archivo `.env`.

---

## âš™ï¸ ConfiguraciÃ³n del Entorno (.env)

En la carpeta `backend`, crea un archivo llamado `.env` con la siguiente estructura:

```env
DEBUG=True

OLLAMA_HOST=http://localhost:11434

LANGSMITH_TRACING=True
LANGSMITH_API_KEY=''

CONVERT_API_HTTP=''
CONVERT_API_SECRET=''

MILVUS_END_POINT=''
MILVUS_API=''

QDRANT_END_POINT=''
QDRANT_API=''
```

### ğŸ“ Notas Importantes:

- Reemplaza `CONVERT_API_HTTP=''` por el **endpoint** proporcionado por ConvertAPI (sin las comillas simples).
- Reemplaza `CONVERT_API_SECRET=''` por tu **clave secreta de ConvertAPI**.
- Para bases vectoriales en la nube:
  - Usa `MILVUS_END_POINT` y `MILVUS_API` si usas Milvus.
  - Usa `QDRANT_END_POINT` y `QDRANT_API` si usas Qdrant.

---
    
## :rocket: InstalaciÃ³n e Inicio del proyecto

1.  **Clona este repositorio**:
    ```bash
    git clone https://github.com/SDiaz0415/IAProyecto.git
    cd IAProyecto
    ```

2.  **Clona este repositorio**

    Para iniciar el chatbot se corre en dos partes, ejecuta el siguiente comando en la consola dentro del directorio del backend:

    ```bash
    make run
    ```

    Una vez iniciado el backend dirigite a la carpeta frontend y ejecuta el siguiente comando en la consola dentro del directorio:

    ```bash
    make run
    ```

3.  **Chat Funcionando**
| App resultado |
|------------------|
| ![app](./backend/data/chat_images/image.png) |
