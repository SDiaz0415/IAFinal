import faiss
import numpy as np
import json
import os
import pickle
from sentence_transformers import SentenceTransformer

# Configuraci√≥n del modelo de embeddings
EMBEDDING_MODEL = "all-mpnet-base-v2"
INDEX_FILE = "faiss_index.bin"
DOCS_FILE = "docs.pkl"
DATA_FILE = "data.json"

class EmbeddingStore:
    def __init__(self):
        self.model = SentenceTransformer(EMBEDDING_MODEL)
        self.index = None
        self.docs = []
        
        if os.path.exists(INDEX_FILE) and os.path.exists(DOCS_FILE):
            self.load_index()
        else:
            self.create_index()

    def create_index(self):
        """Crea un √≠ndice FAISS vac√≠o optimizado con HNSW."""
        self.index = faiss.IndexHNSWFlat(768, 32)
        self.docs = []

    def add_documents(self, texts):
        """Convierte textos en embeddings y los agrega al √≠ndice FAISS."""
        embeddings = self.model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)
        self.index.add(embeddings)
        self.docs.extend(texts)
        self.save_index()
    
    def load_json_data(self):
        """Carga los datos desde el JSON y los indexa en FAISS con mejoras."""
        if not os.path.exists(DATA_FILE):
            print(f"‚ùå No se encontr√≥ {DATA_FILE}. Aseg√∫rate de que el archivo existe.")
            return
        
        with open(DATA_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)

        fragmentos = self.procesar_json(data)
        if not fragmentos:
            print("‚ö†Ô∏è No se encontraron fragmentos adecuados en el JSON.")
            return

        print(f"üìÑ Se extrajeron {len(fragmentos)} fragmentos de texto.")
        self.add_documents(fragmentos)
        print("‚úÖ Fragmentos a√±adidos al √≠ndice FAISS.")

    @staticmethod
    def procesar_json(data):
        """Extrae y optimiza fragmentos de texto para la indexaci√≥n."""
        fragmentos = []

        def recorrer_json(obj, contexto=""):
            if isinstance(obj, dict):
                for clave, valor in obj.items():
                    nuevo_contexto = f"{contexto} > {clave}".strip() if contexto else clave
                    if isinstance(valor, str) and len(valor) > 2:
                        fragmentos.append(f"{nuevo_contexto}: {valor}")
                    elif isinstance(valor, list):
                        for item in valor:
                            if isinstance(item, str):
                                fragmentos.append(f"{nuevo_contexto}: {item}")
                            elif isinstance(item, dict):
                                recorrer_json(item, nuevo_contexto)
                    elif isinstance(valor, dict):
                        recorrer_json(valor, nuevo_contexto)

        recorrer_json(data)
        fragmentos = list(set(fragmentos))  # Eliminar duplicados
        fragmentos = [f.replace("_", " ") for f in fragmentos]  # Mejor formato de texto
        return fragmentos
    
    def save_index(self):
        """Guarda el √≠ndice y los documentos en disco."""
        faiss.write_index(self.index, INDEX_FILE)
        with open(DOCS_FILE, "wb") as f:
            pickle.dump(self.docs, f)

    def load_index(self):
        """Carga el √≠ndice y los documentos desde el disco."""
        if os.path.exists(INDEX_FILE) and os.path.exists(DOCS_FILE):
            self.index = faiss.read_index(INDEX_FILE)
            with open(DOCS_FILE, "rb") as f:
                self.docs = pickle.load(f)
        else:
            print("‚ùå No se encontraron archivos de √≠ndice previos. Creando uno nuevo.")
            self.create_index()

    def search(self, query, top_k=10, umbral=1.0):
        """Busca los documentos m√°s relevantes en FAISS dado un query."""
        if self.index is None or self.index.ntotal == 0:
            print("‚ö†Ô∏è El √≠ndice FAISS est√° vac√≠o. Aseg√∫rate de cargar documentos.")
            return []

        # Generar embedding de la consulta con normalizaci√≥n
        query_embedding = self.model.encode(query, convert_to_numpy=True, normalize_embeddings=True)

        # Buscar en FAISS
        distances, indices = self.index.search(np.array([query_embedding]), top_k)

        # Filtrar por umbral
        resultados = []
        for i, dist in zip(indices[0], distances[0]):
            if i != -1 and dist <= umbral:
                resultados.append(self.docs[i])

        print(f"‚úÖ FAISS activado con top_k={top_k}, umbral={umbral}. Documentos relevantes: {len(resultados)}\n")
        return resultados


# üî• Ejecutar si se llama directamente
# üî• Ejecutar solo si se llama directamente
if __name__ == "__main__":
    store = EmbeddingStore()
    # store.load_json_data()  # Cargar datos desde JSON
    print(f"\nüìå FAISS tiene {store.index.ntotal} documentos indexados.")
    print("üîé Prueba de b√∫squeda:")
    # Definir diferentes queries para evaluar
    queries = [
        "equipo BLOWER?",
        "potencia del equipo",
        "fallas reportadas",
        "pruebas el√©ctricas aislamiento resistencia",
        "tipo de falla sobrecarga recomendaciones repuestos"
    ]

    # Rango de valores para top_k y umbral
    top_k_values = [10, 20, 30, 50]  # Diferentes valores de top_k
    umbral_values = [1.0, 1.02, 1.05, 1.1]  # Diferentes umbrales

    # Iterar sobre cada combinaci√≥n de query, top_k y umbral
    for query in queries:
        print(f"\nüîç **Evaluando query:** {query}")

        for top_k in top_k_values:
            for umbral in umbral_values:
                # Ejecutar la b√∫squeda en FAISS
                results = store.search(query, top_k=top_k, umbral=umbral)

                # Mostrar resultados
                print(f"‚úÖ FAISS activado con top_k={top_k}, umbral={umbral}. Documentos relevantes: {len(results)}")
                print("\nüîé Resultados mejorados:")
                print(results)
                print("-" * 80)  # Separador para visualizar mejor las pruebas

    










    # queries = [
    # "tipo de falla",
    # "servicio realizado en el equipo",
    # "pruebas el√©ctricas realizadas",
    # "empresa responsable de la reparaci√≥n"
    # ]
    # for query in queries:
    #     print(f"\nüîç Buscando: {query}")
    #     print(store.search(query, top_k=20, umbral=1.10))

    # for i, doc in enumerate(store.docs):
    #     if "potencia" in doc.lower():
    #         print(f"üìÑ Documento {i}: {doc}")


   # Ver los primeros 5 embeddings almacenados en FAISS
    # if store.index.ntotal > 0:
    #     query_embedding = store.model.encode(["potencia del equipo"], convert_to_numpy=True, normalize_embeddings=True)
        
    #     distances, indices = store.index.search(query_embedding, 5)
    #     print(f"üîç Distancias encontradas: {distances}")
    #     print(f"üîç √çndices encontrados: {indices}")

    #     # Ver si los √≠ndices corresponden a documentos
    #     for i in indices[0]:
    #         if i < len(store.docs):
    #             print(f"üìÑ Documento encontrado: {store.docs[i]}")
    #         else:
    #             print(f"‚ö†Ô∏è √çndice fuera de rango: {i}")

    # else:
    #     print("‚ùå No hay documentos en FAISS.")




    # # # store.load_json_data()  # Cargar datos desde JSON
    # print(f"\nüìå FAISS tiene {store.index.ntotal} documentos indexados.")
    # print("üîé Prueba de b√∫squeda:")
    # query = "potencia del equipo"
    # results = store.search("potencia del equipo", top_k=10, umbral=0.90)
    # print("\nüîé Resultados mejorados:")
    # print(results)

